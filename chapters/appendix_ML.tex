\chapter*{Appendix: Supervised Learning}
\addcontentsline{toc}{chapter}{Appendix: Supervised Learning}
\label{app:ml}

The supervised learning problem is to generate a mapping from inputs $x$ to outputs $y$ \cite{murphy2012machine}, for example mapping from a subject's fMRI BOLD signal at V1 cortex ($x$) to the identity of an object the subject is seeing ($y$) \cite{dubois2015single}. In the forementioned case, $y$ is a \textit{categorical} variable, which means it has some number $C$ of classes, i.e. $y \in \{1,2,...,C\}$. This defines a \textit{classification} problem. 
A related question may be, given activity on the visual cortex ($x$), to predict the orientation of a bar the subject is seeing ($y$) \cite{carlson2014orientation}. The \textit{target} variable (or \textit{response} variable) $y$ now has a continuum of values, and we name this a \textit{regression} problem.

\section*{Cost functions}
    The problem of supervised learning can be formalized as one of \textit{Function Approximation} \cite{murphy2012machine}. We want to use a dataset of input-output pairs $\{x_i, y_i\}$ to approximate an unknown function $F$ that takes each input into the corresponding output, i.e. $F(x_i) = y_i$. In other words, we want to find a function $F'$ such that new examples $x_n$ will be assigned to their correct value $y_n$. If the rate of right assignments is achieved above chance level, then we can say that the input $x$ is predictive of the output $y$, and that the variables contained in $x$ are informative about the target variable $y$. 
    
    In practice, we want to minimize the difference, or error, between the outputs of the approximated and unknown functions, for which we need some measure of error. Lets call $\hat{y}_i$ the estimated output for input $x_i$. In a regression, we may want to minimize the squared error $S$:
    $$ S = \sum_i [F(x_i)-F'(x_i)]^2 = \sum_i (y_i-\hat{y}_i)^2 $$
    Minimizing the squared error means we penalize larger differences harder, such that we need a hundred one-sized errors to equate a single ten-sized. The squared error rests on the (generally unspoken) assumption that noise is gaussian-distributed, whereas if we want to fit some data in which we have explicit knowledge of the noise distribution, it makes sense to design other penalizations. If noise is exponentially distributed, for example, it is better to minimize the absolute error $A$:
    % Encontrar fonte falando sobre ruido
    $$ A = \sum_i |y_i-\hat{y}_i| $$
    Both squared difference and absolute difference are, in this context, called \textit{loss functions} and measure the error for a single data point $l(y_i, \hat{y}_i)$. When the loss function is summed over the whole dataset, it gives raise to a \textit{Cost function}: Both $S$ and $A$ are cost functions. Because most machine learning algorithms are thought in the domain of \textit{optimization}, Cost function is frequently called referred as \textit{Objective function} \cite[p~79]{goodfellow2016deep}.
    
    In classification problems, classes may lack a canonical ordering, such that it isn't possible to calculate error in the same way as we do in regression. There are specific loss functions for classifications, such as the Cross-Entropy loss:
    %cross entropy. falar um pouco mais sobre as losses para classificação
    \begin{equation}
        C = -\sum_{t=1}^{T}\delta(y-t)log(p(y=t|\hat{y}))
    \end{equation}
    Where $\delta$ stands for Kronecker's delta, which means it is 1 if y equals t and 0 otherwise, and $p(y=t|\hat{y})$ is the predicted probability of the example being of class t.
    
    Machine Learning algorithms differ not only in the choice of objectives, but also in their optimization methods. Common optimization methods include Quadratic Programming, Gradient Descent and Decision Trees, each used in its particular case. For example, Quadratic Programming is used in Support Vector Machines \cite{chang2011libsvm}, Gradient Descent with its many variants are used in Neural Networks \cite{ruder2016overview}, and decision tree methods are commonly applied with boosting \cite{chen2016xgboost}.

\section*{Model Evaluation and Data Partitioning}
% The performance of a model can be unbiasedly estimated via cross-validation, and in this case can be said to give a lower bound on the mutual information $MI(y, x)$ \cite{}.
    
    To evaluate machine learning models, the dataset is generally separated into three partitions, called \textit{train}, \textit{test} and \textit{holdout}\footnote{Sometimes those are called called \textit{train}, \textit{validation} and \textit{test}, in which case the former test case is called validation, and holdout is called test. This latter nomenclature will not be used, to avoid confusion.}\cite{kohavi1995study}. In neuroscience it is common to ignore the holdout set \cite{bakhurin2017differential}, to take advantage of most of the dataset when training, and although this distinction is not important for classifiers for which we don't tune hyperparameters, it may present limitations to analysis of tuned classifiers. When there are many hyperparameters to tune, this tuning procedure approaches a fitting procedure, that is, the fitting of hyperparameters also gets prone to overfitting. Because the metric used to choose hyperparameters is the performance in the test set, it is biased to measure the final performance in the same test set. A holdout set is a subset of the data taken aside, and ignored during all steps before its purposeful use in the final evaluation of performance.
    
    To correctly evaluate the fit of a model, it is necessary to ensure the data used to measure the quality of fit had no part in the fitting procedure \cite{murphy2012machine, kohavi1995study}. However, to make good estimations of the model's performance in the data, its entirety must be part on the estimation. The cross-validation procedure covers both needs, by making multiple fits on different partitions of the data (the train set), evaluating each fit on the remainder (the test set).
    
    There are multiple methods to perform cross validation. The most well-known is called \textit{K-Fold cross validation}. K-Fold works by partitioning the data into K equal sized parts, each destined for one testing, and training the model in the remaining K-1 parts, for a total of K fit-and-evaluate procedures. As an example, a 10-fold cross validation would have 10 fits, each on 90\% of the data. It has the advantage of certainly using every sample of the data for evaluation, but the disadvantage of being rigid on the partition sizes. 
    
    When we must compare performance between datasets of different sizes (e.g. neural activity on the start of training vs after learning), we must ensure training sets are the same size in each, so as to not privilege either. Monte Carlo cross validation accomplishes this by making an arbitrary number of random subsets for training, each having a fixed size \cite{xu2001monte}. In this method a sample may appear in multiple train sets, but the expected number of appearances is always the same for all samples.

\section*{Analysis packages and tutorials}
    In the Python programming language, many classifiers and regressors are implemented in the scikit-learn package \cite{scikit-learn}, with a very user-friendly API, in which classifiers have default configurations for the hyperparameters, enabling fast prototyping of analysis.
    The optimization algorithms have very efficient implementations, and there are functions to help partition the data, to compare results, and to aid in visualization. 