\section{Quality of time representation}

    We use "decodability" as a proxy for quality of time representation. That is: we predict elapsed time from the neural activity, and the correctness of predictions measures how much information about time is contained in the activity. 
    
    For each trial, we analyze the activity from 200ms after nosepoke onset until 300ms before nosepoke removal. Over this interval, the activity from each neuron simultaneously recorded was transformed into firing rate by convolving with a gaussian and then averaging inside bins of 100ms. This procedure resulted in one population firing rate vector per 100 ms time bin, to a total of 10 vectors for 1.5s trials, 25 vectors for 3s trials, and so on. Only the first 10 bins were used for the analysis of trials that had more. 
    
    Elapsed time was decoded from the population firing rates by either classification or regression. While classification requires a population vector to be labeled as one of the 10 time bins, regression treats the output as a continuous function, and can give values such as 450ms or 770ms. 

    Classification was implemented using Linear Discriminant Analysis (LDA) while regression used Bayesian Ridge. While many other algorithms were tested and corresponding hyperparameters tuned, we've chosen to advance our analysis using simpler and untuned algorithms. 

\section{Merging neurons}
    
\section{Bin shuffling}
    Bin shuffling was based on methods from \cite{bakhurin2017differential}. It was used to break the temporal order of population activity, in such a way as to provide a null hypothesis for the neural activity prediction. The population activity 
    

\section{Performance metrics}
    Here we present the metrics used for assessing model performance. The metrics we use depends only on 1 - the final output (prediction) of the model, and 2 - the correct tag. The prediction for all examples is the vector $\hat{y}$, while the correct tag for all the examples is the vector $y$. $N$ is the dimensionality of the vectors. $\bar{x}$ is the sample average of a variable $\sum_{i=1}^N{\frac{x_i}{N}}$. $\sigma$ is the unbiased standard deviation, and $\sigma^2$ is the unbiased variance, $\sigma^2(x) = \sum_{i=1}^N{\frac{(x_i - \bar x)^2}{N - 1}}$.
    
    \subsection{Explained variance}
        Explained variance measures how much the variance in the data decreases when taking the model into consideration. Specifically, it compares the amount of variance in the residuals with the variance in the original data. 
        $$ \text{explained variance}(y, \hat{y}) = 1 - \frac{\sigma^2(y - \hat{y})}{\sigma^2(y)}$$
    
    \subsection{Pearson's r}
        Pearson's r is a measure of linear correlation, calculated by normalizing the linear covariance of two variables by their respective variances. 
        
        $$
        \text{Pearson's r}(y, \hat{y}) = 
        \frac{1}{n-1}\frac{\sum{(y - \bar{y})(\hat{y} - \bar{\hat{y}})}}
             {\sigma(y)\sigma{(\hat{y})}}
        $$
        
        
    
    \subsection{Modified accuracy}
        We modified accuracy to enable its use in regression. Generally, accuracy is ill-defined for continuous targets, since it has no measure of distance. Our modified accuracy consists of rounding the output to its closest value before measuring the accuracy in the conventional sense. The notation $[x]_y$ is the nearest value in $y$ for each value in $x$. $\delta(x_1, x_2)$ is the Kronecker's delta, taking the value $1$ when $x_1 = x_2$ and $0$ otherwise. 
        
        $$
        \text{accuracy}(y, \hat{y}) = \frac{1}{N}\sum_{i=1}^{N}{\delta(y_i, [\hat{y}_i]_y)}
        $$
        
        
        
        
        