%!TEX root = ../dissertation.tex
\chapter{The Brain}
\label{cap:thebrain}

The sensory apparatus of single cell organisms is directly linked to their motor apparatus, meaning that structures capable of detecting perturbations are tightly coupled to structures capable of generating movement \cite[p.~149]{maturana1987tree}. In protozoa, for example, the same flagellum that moves it in the environment also detects obstacles, while some bacteria have chemotaxing mechanisms that change direction of movement, increasing or reducing tumbling rate, in direct response to changes in sugar concentration \cite[p.~147-149]{maturana1987tree}. The coupling between sensory and motor surfaces (i.e. what to do in response to perceived environment), in these direct links, is inflexible %\change{this phrase is unclear. What link is this?}.

In contrast to direct sensory-motor connections, the nervous system appears as an intermediate component between the surfaces of interaction of an organism with the surrounding environment. A nervous system endows its owner with structural plasticity, thus enabling learning via changes in this intermediate step between sensory and motor surfaces, producing bigger behavioral repertoires and complex behavior \cite[p.~175]{maturana1987tree}. This comes at the expense of a lot of energy: up to 20\% of human total resting metabolism \cite{attwell2001energy}, which is used mainly by \textit{neurons} \cite{zhu2012quantitative}, a cell type that receives, modulates, and sends signals. The reason why so much of our energy budget is directed towards this single type of cell, can only be explained by the importance of its activity for the organism.

The introduction will be divided into two parts. In \ref{sec:theory} we introduce some general ideas about the nervous system's role, directing to the idea of representation and into the domain of time perception. Next, in section \ref{sec:da} we give a brief introduction to the functioning of neurons and the type of data which can be collected from their activity, followed by a methodological introduction to the data analysis techniques using supervised learning, applied in this work.

\section{What the brain really does}
\label{sec:theory}
    We know that the nervous system mediates the interaction between a living system's sensory inputs, i.e. what is perceived from the environment, and the system's output, e.g. movement. Based only on excitatory and inhibitory connections between neurons, even in the simplest unidirectional case, it is possible to generate fairly complex behaviors, which can be interpreted as fear responses, aggression, or even logic \cite{braitenberg1986vehicles}. % Possivelmente uma imagem do vehicles, tipo figura 5 pg13
    
    In extremely simple circuits, such as the ones described above, it may be possible to generate mechanistic accounts of this mediation, providing effective explanation to phenomena like reflex arcs. With little complexity added, these explanations can get really intricate, as in the case of the much studied Somatogastric Ganglion (STG) of the lobster. In this system, oscillatory patterns are generated by a circuit of 11 neurons \cite{selverston2009neural}. Despite of this circuit's smallness, it took 30 years from their discovery to the development of mechanistic explanations for its functions \cite{bal1988pyloric, selverston2009neural}. In addition, there are multiple combinations of connections between neurons that give rise to the same rhythms, many of which are found \textit{in natura} \cite{prinz2004similar}. 
    
    % https://www.nature.com/articles/nn1352
    
    This raises an important issue in modern neuroscience, viz. that if we want to increase the predictive power of our models, we have to step away from the implementational level, and delve deeper into theories of brain function and into general properties of brain activity \cite{gerstner2012theory}. One remarkable proponent of this formalism is the Free Energy Principle \cite{friston2009free}: it states how self-organizing systems -- an in special the brain -- can resist the tendency to disorder. According to the Free Energy Principle, to maintain itself in the narrow band of states that are consistent with physiological bounds, an organism must have an internal representation of the environment, constantly updated by new evidence \cite{friston2009free}.

\section{Measurements}
    Neurons communicate via synapses, where action potentials in the pre-synaptic neuron trigger the release of signaling molecules (neurotransmitters), that it will directly or indirectly change the electrical potential of the post-synaptic neuron \cite{purves2014neuroscience}. This change is generally due to the activity of ion channels in the cell's membrane \cite{purves2014neuroscience}, and causes changes in the electrical potential between inside and outside the cell \cite{purves2014neuroscience}, creating dipoles and thus electric fields in the region.
    
    % Neurons fire action potentials when their membrane is depolarized enough to open voltage-dependent ion channels. 
    When measuring the electric potential from afar (e.g. using EEG in the scalp), only the summed activity of huge collectives of neurons acting in synchrony can be detected, and it is hard to pinpoint the origin of the signal \cite{buzsaki2012origin}. On the other hand, intracranial electrodes can be placed inside regions of interest in the brain, to detect changes in the electric field in the scale of millimeters and microsseconds \cite{}. The measured activity is then called the Local Field Potential (LFP), and while it contains information about the collective activity of aggregates of neurons \cite{buzsaki2012origin}, we can extract from it activity from individual neurons, in specific locations. To this end we use a process named spike-sorting \cite{rey2015past} that extracts the times of action potentials from the voltage timeseries of each electrode. 
    
    The firing of a neuron usually depends on unobservable activity and inner states, such as concentration of proteins \cite{}, and more generally short-term plasticity \cite{motanis2018short}. This makes the sequence of action potentials that are caused by some event to vary from trial to trial, even in cases where the neuron has a known tuning to the event \cite[p~7-8]{dayan2001theoretical}. To account for this lack of knowledge, neural activity is treated probabilistically, by switching from the spike times to the underlying firing rate \cite[p~9-11]{dayan2001theoretical}. The firing rate can be calculated by counting the spikes in sliding windows, or using kernel convolutions with any kernel having both unit area and a given temporal resolution, an example of which is the gaussian kernel \cite[p~9-11]{dayan2001theoretical}.
    
    \subsection{Machine Learning}
        Although the very early days of machine learning are deeply intertwined with neuroscience \cite{mcclelland1986parallel}, much of past years neuroscience was studied with univariate techniques. Recently, the surge of multi-voxel pattern analysis (MVPA) in fMRI brought machine learning back into the field \cite{}, later rebranded as Multi Variate Pattern Analysis and finding ever more applications \cite{haxby2012multivariate}. 
        
        Machine learning is a big field, with many techniques aimed at multivariate problems. The kind being used in MVPA and more frequently in neuroscience is called Supervised Learning (see the Appendix for a more thorough presentation). Supervised learning aims to disclose - or \textit{predict} - some hidden attribute of data points based on observable attributes - or features. These predictions are used in the industry in the more varied forms, inside the company to make financial decisions, and inside products to change usability. In neuroscience, on the other hand, it is generally used in conjunction with other statistical techniques, to test hypothesis about this underlying attributes. For example: To test whether activity in the visual cortex contains information about a given stimulus, machine learning can be used to predict the stimulus based on the activity. If predictions are above chance, then there was information contained in the activity.
        
        %\cite{merchant2008we} % MVPA temporal generalization 

\section{Levels of analysis}
    There are multiple manners of studying cognition and brain function, beyond the search for neural correlates. To enable them fruitful advances, unlimited by technical limitations, Chomsky proposed we should develop cognitive models without regard to biological implementation - even though these mechanisms should be worked out at a later stage \cite[p.~12]{chomsky2006language}.

    The necessity to divide the study of the nervous system into levels of analysis took some time to establish itself. Confronting the limitations of finding neural correlates for phenomena, Marr articulates how, even if we did found the "apocryphal grandmother cell", it would not tell us "why or even how such a thing may be constructed from the outputs of previously discovered cells" \cite[p~15]{marr1982vision}. It is true that much has changed since then, especially with respect to the understanding of \textit{how}\footnote{In the case of vision, derivatives from the distributed processing framework eventually gave rise to algorithmic models of the visual cortex \cite{fukushima1980neocognitron}, and the building up of complex cells from simpler ones is now understood and emulated \cite[p~??]{goodfellow2016deep}.}, but Marr's case is clear: multiple characterizations are needed to account for brain phenomena. 
    
    \begin{quote}
    The key observation is that neurophysiology and psychophysics have as their business to \textit{describe} the behavior of cells or of subjects but not to \textit{explain} such behavior. What are the problems in doing it that need explaining, and at what level of description should such explanations be sought?
    \end{quote}
    
    To understand some brain function, it does not suffice to understand its hardware implementation - i.e. the neuronal activity and connections. First of all it is necessary to have a precise understanding of what is to be computed. This knowledge makes possible to devise representations and algorithms that are sufficient to compute what is needed, and at last biological mechanisms that implement those algorithms and representations. The levels are not detached from one another, but they are loosely related: some characteristic biases may be explained at only one or two of them. 
    
    \begin{table}[]
        \centering
        \begin{tabular}{p{4cm}p{4cm}p{4cm}}
        \hline \vspace{.2cm}
            Computational Theory & Representation and algorithm & Hardware implementation\\\hline
            
            What is the goal of the computation, why is it appropriate,and what is the logic of the strategy by which it can be carried out? & 
            
            How can this computational theory be implemented? In particular, what is the representation for the input and output, and what is the algorithm for the trans formation? &
            
            How can the representation and algorithm be realized physically?
            \\\hline
        \end{tabular}
        \caption{The three levels at which any machine carrying out an information-processing task must be understood. From \cite{marr1982vision}, figure 1-4}
        \label{tab:three_levels}
    \end{table}
    
    Marr advocates centrally that we need Computational understanding of a system to conduct the lower level investigations. In the case of vision, he proposed the computational role of "extracting invariant representations", building upon the preceding work that had just started discussing the separation of reflectance and illumination by the visual system, and how this could be accomplished. It is a change of perspective from "what may the system be doing" to "how may the system do this".
    
    \subsection{The cash register information-processing system}
    
        Marr exemplifies the framework by the cash register, that does the process of addition. The process maps two symbols into a single one; has rules such as commutativity and associativity, that ensure the process is not affected by ordering; and has inverse and null elements, allowing to subtract quantities or keep them the same. The rationale for these rules is obvious in a setting such as a cash register, where we want no effect of the product ordering, and want to pay nothing if we take nothing. This concludes the computational role of the cash register, explaining \textit{what} it does and \textit{why}, and is "true no matter how the numbers are written -- whether in binary, Arabic, or Roman representation".
        
        Next, the second level explains \textit{how} such a function is performed in the system, from the choice of representation to the algorithm performed upon this representation. Both are codependent: specific representations facilitate the use of some specific algorithm and difficult the use of some other. Roman numerals, for example, can be used to represent numbers, but they make addition harder (and nevermind multiplication). The cash register, in Marr's time, used decimal representation for the numbers, and the same algorithm used by students in basic school, going digit-by-digit from right to left and carrying digits. The same representation and algorithm that is implemented by ink and paper can be implemented by the wiring and transistors.
        
        This takes us to the lower level, in the sense of the specific hardware in which is implemented the algorithm that performs the function. This is the level from which most neuroscience research parts from, asking questions such as what are the possible functions performed by neural circuitry, or in which way neurons could perform some specific function.
    
    \subsection{Computational models in neuroscience}
        
        Models in the computational level have a specific sense in this framework. They define precisely the 
        % FEP
        % Outros existem?
        % Reinforcement Learning
        
        For timing, 
        
        Recently, the biggest contributor to Marr's ideas, Tomaso Poggio, proposed that we should add two levels of explanation to the original three. The level of \textit{learning} should explain how a given function and representation is developed by an organism, and \textit{evolution} should explain how this learning came to be \cite{poggio2012levels}. 
        
        In the present work we will try to discuss the three levels in addition to learning
        % \cite{leopoldo2018computational}
        
\section{Representation}
\label{sec:representation}
    The notion that representation is central to cognition is an old one \cite[p.~134-140]{rosch1991embodied}, and it grounds one central method for assessing brain function: the search for neural correlates of external quantities. We know that there are neurons in the primary visual cortex that represent (i.e. encode) inclination angles of bars \cite[p.~13]{dayan2001theoretical} and neurons in the primary motor cortex that represent the angle of reaching movements \cite[p.~14]{dayan2001theoretical}, but more complex concepts, like one's grandmother \cite{} or causality \cite{}, may have inner workings too context-dependent to capture with localized correlates. 
    
    Since the early days of neuroscience, there is a debate about the general form of brain representation. Cognitivists posed that representation is formed by well defined strings of symbols that logically interact, while connectionists maintain that representation is distributed in the neuronal connections. The former vision is exemplified in the seminal McCulloch \& Pitts paper \cite{mcculloch1943logical}, where they propose that "To each reaction of any neuron there is a corresponding assertion of a simple proposition". The latter is commonly exemplified by modern machine learning \textit{neural networks}, where single neurons rarely have some \textit{meaning} on their own. Connectionism gained strength in the late 80's by the seminal Parallel Distributed Processing \cite{mcclelland1986parallel}, and although many of its related algorithms fall short of biological plausibility, there are those such as Hopfield Networks and Boltzmann Machines, based on autoassociative rules reminiscent of Hebbian learning, that made their way into modern models of brain function \cite{}. 
    
    The debate between Cognitivism and Connectionism cuts across levels of analysis, making it specially intricate. Implementational connectionists pose that the views are consistent with one another, with the brain's neural network acting as a distributed system that, when analyzed through a higher level of abstraction, is a symbolic processor \cite{}. Alongside this theoretical studies, measurements of neural activity are generally studied through the lens of representation, with the most prominent being single neuron representations \cite{}, mainly consistent with the Cognitivist view.
    
    Single neuron representations have been specially studied by the vision community \cite{deyoe1988concurrent,bell1997independent,ito2004representation,lee2008sparse}, but recent contributions have taken this approach much beyond the sensory realm, to the more abstract domains of \textit{space} and \textit{time} \cite{eichenbaum2014time}. \textit{Place cells} are neurons that fire in specific locations of some environment \cite{foster2006reverse}, and are mainly present in the Hippocampus \cite{o1979review}, bestowing the brain with a whole new dimension along which to relate its activity \cite{eichenbaum2014time}. In the same way, \textit{time cells} fire in specific times after some event \cite{tiganj2016sequential, eichenbaum2014time}, and may serve as temporal basis for representations of the world \cite{ludvig2008stimulus}, being complemented by \textit{ramping neurons}, that linearly increase or decrease their activity during a time interval \cite{morrison2009convergence, kim2013neural, tiganj2016sequential, parker2016timing}.
    
    In becomes clear, through the previous paragraphs, that the search for neural correlates deeply entangles the representational and implementational levels, and may be better understood through some distancing. Representations have to do with the internal objects that are transformed through some algorithm, for which many implementations are possible. Take the following example: time is a quantity that evolves monotonically and linearly, and so does its representation. The monotonicity can be achieved by any phase-space trajectory, with the condition that it does not cross itself. In addition, linearity can be achieved by a simple nonlinear regression reading-out the monotonic activity. The regression could be implemented having another general trajectory as output \cite{} or a single ramping neuron: The choice of input and output is by no means obvious, and depend at least as much in the computational goals as the implementational constrains.
    
    Although there are many examples of quantities encoded in single neurons, such as aforementioned, it is not necessary that particular neurons have such interpretable roles. Phase space trajectories can be composed by the activity of populations \cite{shamir2014emerging, quiroga2009extracting}, as well as less explicit dimensions such as post-synaptic sensitivity \cite{motanis2018short}, and neuromodulation effects \cite{friston2009free, friston2010free}. While the latter two also affect behavior \cite{wolff2017dynamic}, we will not account for them in the present work, as they are not directly detectable from neural activity. Population activity, on the other hand, will be discussed.
    % accumulators bueti2011physiological, wittmann2010accumulation
    
    Population codes have the capacity to carry much more information than single neuron codes \cite{hardy2016neurocomputational}, but since population patterns are by definition multivariate, their study requires more elaborate techniques \cite{quiroga2009extracting}, such as Machine Learning. There are many ways in which a given neural population may encode information \cite{quiroga2009extracting, shamir2014emerging, mello2015scalable}, and in the specific case of encoding temporal information, several models have been developed to account for characteristics and biases present in empirical data \cite{hardy2016neurocomputational}. The motivation and characteristics behind some of these models will be presented below.
    